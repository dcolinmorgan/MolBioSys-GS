\section{Methods and GeneSPIDER capabilities}
\label{sec:methods}
The functions in the following sections cover the five typical phases in benchmarking of inference methods based on \insilico experiments: 
Generation of network models (phase A), design of perturbation experiments (phase B), simulations of the designed experiments on the generated systems (phase C), inference of network models from the simulated datasets using the methods of interest (phase D), and analysis of the performance of the methods (phase E).

\subsection{Model formalism and notation}
\label{sec:model_formalism}

A linear dynamical system described by ordinary differential equations (ODEs),
\begin{equation}
  \begin{array}{r c l}
    \frac{d}{dt} \check{x}_i(t) &=& \sum_{j=1}^N \check{a}_{ij} \check{x}_j(t) + p_i(t) - f_i(t)\\
    y_i(t) &=& \check{x}_i(t) + e_i(t),
  \end{array}
  \label{eq:linearsys}
\end{equation}

\noindent
is used to approximate the local behaviour of a biological system in the current version of \gs.
This approximation is commonly used to describe GRNs, see \eg \citet{Yuan2011,Gardner2003}.
In GRNs the state vector \(\check{\bx}(t)=[\check{x}_1(t),\check{x}_2(t),\ldots,\check{x}_N(t)]^T\) represents actual mRNA expression changes relative to the initial state of the system in \(\breve{N}\) genes.
The vector \(\bp(t)=[p_1(t),p_2(t),\ldots,p_N(t)]^T\) represents the applied perturbation, which, in general, is corrupted by the noise \(\bbf(t)\).
The perturbations could be \eg gene siRNA knock-downs or gene over-expressions using a plasmid with an extra copy of the gene.
The response vector \(\by(t)=[y_1(t),y_2(t),\ldots,y_N(t)]^T\) represents the measured expression changes, which is the ``true" expression corrupted by the noise \(\be(t)\).
Our network is the interaction matrix with parameters \(\check{a}_{ij}\), which represent the influence of an expression change of gene \(j\) on gene \(i\).
A positive value of \(\check{a}_{ij}\) represents an activation, while a negative value represents an inhibition.
The value of the \(\check{a}_{ij}\) parameter gives the relative strength of the interaction.
Here \(\; \check{} \;\) is used to mark ``true" variables that are not corrupted by noise or measurement errors.


\subsection{Network generation}
\label{sec:network_generation}

Network generation (phase A) is done in four steps: (i) generation of a network graph with desired topology, (ii) assignment of random weights to the links, (iii) stabilization of the system, and (iv) tuning of the IAA degree of the system corresponding to the network.

\gs includes three algorithms for directed and undirected network topology generation (step i): random, scale-free, and small-world; auto-regulation/degradation self-loops are supported for directed networks.
The desired number of links is set by providing a sparsity parameter to the algorithm. The sparsity is defined as \(s=L/N^2\), where \(L\) is the number of links in the network and \(N^2\) the number of possible links in a directed graph.
The algorithm generating random graphs, where each link has the same probability \(p_l\) to exist, is based upon the assumption that every graph, \(G(N,L)\), is equally probable \citep{Erdos1961}.
The algorithm generating scale-free graphs \citep{Barabasi1999}, where each node added to the network is attached to previous nodes with a probability \(p_l\) proportional to the number of edges each of the previous nodes has, is based upon the preferential attachment principle \citep{Reka-Barabasi2002}.

This algorithm produces a network with bidirectional links only. 
However, directed biological networks rarely have bidirectional links, so we at random eliminate one of the links in each symmetric link pair with probability \(p_{\text{remove}}\), implying that we keep both with probability \(1-p_{\text{remove}}\). When eliminating a link we eliminated the outgoing link $a_{ji}$ with probability \(p_{\text{outgoing}}\).

The algorithm generating small-world graphs \citep{Watts1998}, where nodes are serially connected with the \(l\) closest neighbours in a ring lattice, is based on the implementation suggested by \citet{Prettejohn2011}. Each node is then considered to randomly be disconnected from a neighbour and reconnected to another node in the ring lattice.
Graphs that are partly scale-free can be generated by seeding the preferential attachment algorithm with \eg a small-world network so that a small-world-scale-free (SW-SF) network is generated.

After the topological layout, a weight is assigned to each link present by assigning weights drawn from a iid normal distribution with mean zero and standard deviation one (step ii).
This is done in order to convert the graph into a dynamical system. The weight \(\check{a}_{ij}\) of each link that is present determines the effect that node \(j\) has on node \(i\).

% \textcolor{red}{
Since a stable system guarantees that all quantities remain finite, a stable network model is typically desired. The weighted network may therefore need to be stabilized (step iii) and have its condition number tuned, \ie tuning of the interampatteness (IAA) degree \citep{Nordling2009}. IAA is a generic property of biochemical networks, akin to the data property collinearity.%}
When assigning random weights to each link there is no guarantee that the resulting system is stable. Stability is guaranteed if all eigenvalues of the interaction matrix are negative. 
Data generation for benchmarking of inference algorithms should be restricted to stable systems, because, while the local linearisation of a globally stable nonlinear system can be unstable \citep{Khalil1996}, it is not possible to infer an unstable linear system in practice; even the smallest amount of process noise will move the system away from the unstable steady-state.
Note that every isolated system must be globally stable, or risk some state variable (mRNA abundance) becoming infinite and violating mass-conservation laws.
We assure stability by employing a method that forces all eigenvalues of our interaction matrix to be negative\citep{Zavlanos2011}, enabling the creation of a stable linear dynamic system from most unstable network graphs.
However, this does not work for all graphs because the algorithm is based on making the diagonal dominating and negative, whereupon addition of a few links typically solves the problem.
% \textcolor{red}{
The algorithm forces the eigenvalues to be negative by incorporation of a convex optimization protocol\citep{grant2008cvx}. While  convexity guarantees that the algorithm for each initial network returns only one final network, two different initial networks may yield the same final network.
Desired network properties can be arrived upon by iterating over the substantially reduced space of stable linear dynamical systems.%}

The IAA degree tuning (step iv) is enabled by the user defining limits on the eigenvalues of the network in the convex optimization.
This sets an upper bound on the inverse condition number, which can be infeasible or arbitrarily conservative.
In practice, one therefore needs to generate a network ensemble with different random topological weights and select those of condition number (IAA degree) within the desired interval. Note that links are removed if their strength falls below a user defined threshold.
This ensures that we keep link strengths within a reasonable range to avoid trivial links as well as possible numerical issues when working with the network in subsequent steps.
However, this can result in a slight deviation from the desired sparsity.

\subsection{Data generation}
\label{sec:data_generation}


The purpose of the experiment design (phase B) is to generate a perturbation matrix \(\mP\), containing either a sequence of perturbations in a time-series experiment or a sequence of steady-state experiments. 
Only a small number of genes should be perturbed in each perturbation, so that it may be practically implemented \invivo or \invitro given current technical limitations.
Publicly available real gene expression datasets have either been generated by perturbing one gene at a time (single gene perturbation), two genes at a time (double gene perturbations), or by a system perturbation, such as a change in environmental factors or introduction of a drug, which is thought to affect several genes at once but typically is quantitatively unknown.
These perturbations are trivial to generate in \matlab using \eg the function for generation of diagonal random matrices \texttt{diag(randn(N,M))} or sparse random matrices \texttt{sprandn(N,M)}.

We also provide a method to generate a pseudo-optimal perturbation design that counteracts intrinsic signal attenuation based on an inversion of the ``true" system. 
Small elements in the designed perturbation matrix \(\mP\) are removed to make it sparse, while keeping the condition number of the response matrix \(\kappa(\mY)\)  close to 1.
This implies that the response matrix will be close to an identity matrix, with equal signal strength in \(N\) linearly independent directions.
Note that while this method cannot be used for biological experiments (because the ``true" system is unknown), it can be used to generate informative datasets for benchmarking.

The experiments are simulated (phase C) either by calculating the steady-state response, as in equation (\ref{eq:Linearmap}), or the time-series response, as in equation (\ref{eq:linearsys}).
The system dynamics are simulated with a time step \(t = \tau_N/10\), where \(\tau_N\) is the smallest time constant of the system given by the eigenvalues of the network matrix.
We typically first simulate noise-free measurements and then add noise drawn from the desired distribution, typically a standardized normal distribution with zero mean and variance one.
This enables us to tune the SNR by scaling the noise matrix, which for normally distributed noise corresponds to scaling of the standard deviation, such that the desired SNR is obtained. 

\label{sec:data-analysis}

\subsection{GRN inference methods}
\label{sec:inference-methods}
A core piece of any inference pipeline is the inference method itself, which takes data and algorithm parameters as input and yields one or several network estimates as output (phase D). In \gs we have created wrappers with a common entry format for a number of published and in-house inference methods to simplify their use, see Table \ref{tab:gs-methods}. Wrappers make it easy to incorporate future inference algorithms and provide the same benchmark capabilities in our standardised environment.

We here introduce a modified version of the Robust Network Inference algorithm (RNI)\citep{Nordling2013phdthesis} by applying a range of cut-offs to the confidence scores, and call this version RNI cut-off (RNICO). This way different network estimates ranging from empty to full are obtained by gradually lowering the confidence cut-off independent of how informative the data is, while native RNI for uninformative data produce an empty network.  
The significance level \(\alpha=0.01\) was used in RNICO to scale Nordling's confidence score such that every link with a value above one exists in the true network for 99 out of every 100 links fulfilling this criterion. 

\subsection{Evaluation of inferred networks}
\label{sec:evaluation_of_networks}

Measures of network similarity are needed to assess the performance of any given inference method (phase E), and we have decided to implement many such measures to accommodate researchers' preference.
\gs currently provides a function for comparison of networks that includes 18 system measures, 3 signed topological measures, 1 correlation measure, 12 graph measures, and 9 directed graph measures, see section \ref{sec:compare-models}.
The \gs toolbox includes many standard measures of performance, including MCC and AUROC, with which to compare available inference pipelines and tune such for specific dataset properties.


\subsection{Quantification of network and system properties}
\label{sec:quantification_network_properties}

To investigate if a relation exists between a network property and \eg the performance of an inference algorithm, the network property first needs to be objectively quantified and a measure of it implemented.
\gs contains functionality for the calculation of basic graph properties, such as number of nodes and links, basic topology measures reported in \citet{Prettejohn2011} such as the clustering coefficient, the degree distribution, the average path length, and advanced measures such as the number of strong components. We calculate these measures for each network structure based on the network type, either directed or undirected. We have also implemented functions for calculation of system properties, such as the time constant and IAA degree.


\subsection{Quantification of data properties}
\label{sec:quantification_data_properties}

For the expression data we have tried to quantify the informativeness and difficulty to infer the correct network in terms of signal to noise ratio, similarity between experiments, and the condition number.
\gs contains several different SNR calculations; see definitions in section \ref{sec:SNR}. The condition number is calculated with the \matlab function \texttt{cond}.